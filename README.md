# Green IT - Dynamic Network Topologies
## Overview
The files contained in this repository are designed to test the implementation of a dynamic topology mechanism that performs the following:

* Based on overall network traffic, selects nodes to place into a standby state

* In the standby state, each node maintains one connection to a node in the active state, but shuts down all other connections. The intent is to make no routing decisions and reduce power consumption; all non-local traffic is forwarded from the standby node to the active node

The “Dissertation.pdf” document provides a full explanation of the research, development, and implementation of this project.

## Operation
There are three components in the dynamic topology mechanism:

* A centralized controller

* A collection of network nodes

* The host devices connected to the network nodes

These entities interact in the following loop:

1. Nodes: Send network traffic information to Controller

2. Controller: Analyze network traffic matrix and determine optimal topology

    * Focus on energy consumption reduction through the use of node standby states
    
    * Ensure link utilisation does not rise above a threshold value for any of the links
    
    * Apply optimisation heuristics

3. Controller/Nodes: Send/fetch the topology change information

4. Nodes: Implement the topology change

5. Repeat

For testing purposes, each network node contains a virtual machine that generates traffic to simulate a locally connected network. The full logical topology, including the virtual hosts running on each node and the centralized controller, is shown below:
<div align="center">
    <img src="topology_logical.png" width="600px"</img>
</div>
<br>
The inter-node traffic generated by the hosts is designed to be classified as high, medium, or low load, and the controller correspondingly selects one of the three topologies shown below (grey nodes and links are in the standby state):

* **High traffic load**: No nodes are in the standby state

<div align="center">
    <img src="topology_high.png" width="470px"</img>
</div>
<br>

* **Medium traffic load**: Nodes 1, 2, and 4 are in the standby state

<div align="center">
    <img src="topology_med.png" width="470px"</img>
</div>
<br>

* **Low traffic load**: Nodes 1, 2, 4, 5, and 6 are in the standby state

<div align="center">
    <img src="topology_low.png" width="470px"</img>
</div>
<br>

## Implementation

The full implementation details can be found in Chapter 4 of the “Dissertation.pdf” document.

### Controller

The centralized controller alternates between collecting traffic data from each of the nodes and using the data to determine the optimal topology at that point in time. The controller acts as an sFlow collector, and the sFlow messages sent by each node are used to construct the matrix of inter-node traffic flows. 

#### Main program
1. Define measurement interval and topology configuration filename, and declare variables 
2. Initialise adjacency matrix with full topology
3. Capture measurement start time
4. Enter infinite loop
	1. Clear sFlow data from last captured message 
	2. Get sFlow message from stream input 
	3. Process sFlow message 
		1. Dump unwanted sFlow message fields 
		2. Get the node number of the message agent 
			* Continue to next sFlow message capture if the node number is not one of the expected values. 
		3. Dump unwanted sFlow message fields 
		4. Get the node number of the traffic’s source 
			* Continue to next sFlow message capture if the node number is not one of the expected values. 
		5. Dump unwanted sFlow message fields 
		6. Get the node number of the traffic’s destination 
			* Continue to next sFlow message capture if the node number is not one of the expected values 
		7. Dump unwanted sFlow message fields 
		8. Get the traffic’s packet size 
		9. Dump unwanted sFlow message fields 
	   10. Get the sFlow sampling rate 
	4. If the agent and destination nodes are equal, multiply the packet size by the sample rate and add it to the current traffic count for the corresponding source/destination pair. 
	5. Measure the time difference between now and the measurement start time 
	6. If the time difference is greater than the measurement interval, construct the traffic matrix and optimise the topology 
		1. Capture a new measurement start time 
		2. Use the current and previous traffic counts, the current and previous time differences, and equation 4.1 from the dissertation to calculate the traffic demand for every source/destination node pair 
		3. Store the current traffic count and time difference as the previous traffic count and time difference 
		4. Display the traffic matrix at standard output 
		5. Create a new, empty topology configuration file 
		6. Run the topology optimisation algorithm (shown below) and write the output to the new file 
		7. Compare the new topology with the current topology 
		8. If the files are identical, delete the new file, otherwise, overwrite the old file with the new file 
	7. Return to the start of the loop

#### Optimisation algorithm
1. Store the traffic matrix, adjacency list, output filename, and dynamic topology mechanism deactivation switch state (all passed by value from calling function) 
2. Determine number of nodes in the network, initialise link utilisation threshold, and declare variables
3. Set all nodes to be active
4. Calculate shortest path for each source/destination node pair using Dijkstra’s algorithm
5. Use the traffic matrix and the shortest paths to determine the traffic load on each link
6. Calculate the traffic loading on each node as the sum of the traffic load on all connected links
7. Use the link traffic load and link bandwidths to calculate the link utilisation percentage
8. Find maximum link utilisation percentage
9. If the dynamic topology mechanism is not disabled, loop while the maximum link utilisation is less than the threshold value 
	1. Break the loop if attempts have been made to place each node in standby 
	2. Clear the previously calculated node and link traffic loads 
	3. Set the variable that tracks the reachability of the nodes to false (guarantees at least one execution of the loop below) 
	4. Loop while the link utilisation is above the threshold or one or more nodes are unreachable 
		1. Use the shortest paths to determine the frequency of each node’s use as a transit node 
		2. From the set of nodes that are candidates for standby, find the node that is used as a transit node the least. Use the nodes’ traffic load to resolve any conflicts 
		3. Break the loop if a least used node cannot be determined 
		4. Remove the node from the set of nodes that are candidates for standby 
		5. Store the node’s adjacency information. This is only used if placing the node in standby has to be reversed 
		6. From the set of nodes adjacent to the node to be placed in standby, find the node that is used as a transit node the most. Use the nodes’ traffic load to resolve any conflicts 
		7. Update the adjacency matrix of the node to be placed in standby and all its adjacent nodes 
		8. Recalculate shortest paths for new topology 
		9. Recalculate the traffic load on each link 
		10. Recalculate the link utilisation percentage 
		11. Find maximum link utilisation percentage 
		12. Test if all nodes are reachable from every node 
		13. If the link utilisation is above the threshold or one or more nodes are unreachable, reverse the actions taken to modify the adjacency matrix and return to the start of the loop 
	5. Recalculate the traffic load on each link 
	6. Recalculate the link utilisation percentage 
	7. Find maximum link utilisation percentage 
	8. Return to the start of the loop if the maximum link utilisation is less than the threshold value 
10. Write the next hop in the shortest path for each source/destination node pair to the specified output file

### Node

Each of the nodes must perform initialization operations to ensure their components interact correctly internally and with the rest of the topology. These components include Open vSwitch for topology implementation, NTP to synchronize changes, host container for traffic generation. Once initialized, the node simply checks for topology changes and implements them if appropriate.

#### Main program 
1. Define interrupt handler that removes Open vSwitch configuration and deletes the local topology configuration file 
2. Define the time to wait between modification time checks, the controller’s URL to pass to cURL, and the configuration filename and declare variables
3. Determine the current node by examining the last character of the hostname
4. Force the software clock to synchronise with the controller’s NTP server 
5. Start and initialise the LXC container and Open vSwitch
6. Enter infinite loop
	1. Use cURL to get the modification time of the controller’s topology configuration file 
	2. Test if the remote file’s modification time is later than the local file’s modification time
	3. If the remote file is newer: 
		1. Attempt to get the topology configuration file from the controller 
		2. If unsuccessful, wait for half the defined period of time and return to the start of the loop 
		3. If successful, update the modification time of the local topology configuration file and implement the topology change 
	4. If the remote file is not newer: 
		1. Wait for the defined period of time 
	5. Return to the start of the loop

#### Initialisation 
1. Store the current node number (passed by value from calling function) 
2. Check for presence of PID files to determine if Open vSwitch is running
	* If Open vSwitch is not running, start it 
4. Check if the LXC container is running 
	* If the LXC container is not running, start it 
6. Get the MAC address of the LXC container’s interface using the ARP cache 
7. Get the MAC address of the node’s interface that connects to the LXC container using the interface’s address file 
8. Add the default output handling flow tables to Open vSwitch 
9. Add the transit and terminating traffic processing flows to flow table zero of Open vSwitch 
10. Modify the output handling flow table for terminating traffic using the node current number and LXC container and node MAC addresses 
11. Add the originating traffic flows to flow table zero of Open vSwitch using the current node number 
12. Add the firewall flow to flow table zero of Open vSwitch

#### Topology change implementation 
1. Store the topology configuration filename and current node number (passed by value from calling function) 
2. Declare variables 
3. Get current time 
4. Determine topology change time 
5. Retrieve the next hop node number for each destination node from the configuration file 
6. Use the interfaces’ IP addresses to determine which node each interface connects to 
7. Convert the next hop node number from the configuration file into an interface number for each destination node 
8. Convert the interface number into an Open vSwitch port number for each destination node 
9. Wait until the local time equals the topology change time 
10. Modify the output processing flow tables using the port numbers for each destination node

### Host
The nodes each have an LXC container that simulates hosts on the node’s local network, which are used to generate the inter-node traffic and run the tests to measure the impact of the dynamic topology mechanism.

#### Traffic generation
1. Define the traffic generation matrix filenames to be used and declare variables 
2. Use the last character of the hostname to determine the node the host is connected to 
3. Extract the traffic generation data relevant for this host from the traffic generation matrix files for all three testing intervals 
4. Determine the local time and test start time to synchronise with other nodes 
5. Wait until the start time 
6. Prepare for traffic reception from other hosts 
7. For each of the three testing intervals 
	1. Start traffic generation to all other host using the traffic generation matrix that corresponds to this testing interval. The traffic generation is set to run for an integer multiple of the testing interval; the first matrix is run for three intervals, the second matrix is run for two intervals, and the third matrix is run from one interval. 
	2. Start the ping to all other hosts. The ping is set to run for one testing interval 
	3. Wait for one testing interval, then progress to the next testing interval. If all tests have been performed, end

#### Performance measurement 
1. Define the file numbers of the iPerf data of interest and declare variables 
2. For every iPerf statistic file, dump unwanted data and extract the jitter, loss and out of order packet statistics 
3. For every ping statistic file, dump unwanted data and extract the average round trip time 
4. Print the collated data to standard output

### Results
The impact of the Dynamic Topology Mechanism (DTM) in comparison to OSPF and MPLS implementations in the same set of nodes in the same traffic conditions is shown below using the metrics of delay, jitter, packet loss, out of order packets, and the accuracy of the measurement of inter-node traffic demands. A full analysis and discussion of the results can be found in chapter 5 of the “Dissertation.pdf” document

**Delay**

<img src="https://github.com/costa4487/Dissertation-2015/blob/master/delay.png" width="470px" align="middle">

**Jitter**

<img src="https://github.com/costa4487/Dissertation-2015/blob/master/jitter.png" width="470px" align="middle">

**Packet loss**

<img src="https://github.com/costa4487/Dissertation-2015/blob/master/loss.png" width="370px" align="middle">
				     
**Out of order packets**

No out of order packets were observed in any of the test cases for any implementation

**Traffic demand measurement**

<img src="https://github.com/costa4487/Dissertation-2015/blob/master/demands.png" align="middle">
